{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdpPHz2Lp6VD"
   },
   "source": [
    "# TP Coding an autoencoder in Pytorch\n",
    "\n",
    "Author : Alasdair Newson\n",
    "\n",
    "alasdair.newson@telecom-paris.fr\n",
    "\n",
    "## Objective:\n",
    "\n",
    "The goal of this TP is to explore autoencoders and variational autoencoders applied to a simple dataset. In this first part, we will look at an autoencoder applied to MNIST. We recall that an autoencoder is a neural network with the following general architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TH9YkFsrsidu"
   },
   "source": [
    "![AUTOENCODER](https://perso.telecom-paristech.fr/anewson/doc/images/autoencoder_illustration_2.png)\n",
    "\n",
    "The tensor $z$ in the middle of the network is called a __latent code__, and it belongs to the latent space. It is this latent space which is interesting in autoencoders (for image synthesis, editing, etc).\n",
    "\n",
    "### Your task:\n",
    "You need to add the missing parts in the code (parts between # --- START CODE HERE and # --- END CODE HERE or # FILL IN CODE)\n",
    "\n",
    "\n",
    "First of all, let's load some packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JqNeIJ8Op8Ao"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import pdb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def pytorch_to_numpy(x):\n",
    "  return x.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hyj5dj_eui9D"
   },
   "source": [
    "Now, we load the MNIST dataset, which we will use for training a simple autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4YPLKlPrufSk"
   },
   "outputs": [],
   "source": [
    "# MNIST Dataset\n",
    "mnist_trainset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "mnist_testset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "#create data loader with smaller dataset size\n",
    "max_mnist_size = 1000\n",
    "mnist_trainset_reduced = torch.utils.data.random_split(mnist_trainset, [max_mnist_size, len(mnist_trainset)-max_mnist_size])[0] \n",
    "mnist_train_loader = torch.utils.data.DataLoader(mnist_trainset_reduced, batch_size=256, shuffle=True,drop_last=True)\n",
    "\n",
    "# download test dataset\n",
    "max_mnist_size = 512\n",
    "mnist_testset_reduced = torch.utils.data.random_split(mnist_testset, [max_mnist_size, len(mnist_testset)-max_mnist_size])[0] \n",
    "mnist_test_loader = torch.utils.data.DataLoader(mnist_testset_reduced, batch_size=256, shuffle=True,drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7YhlBT2PN9I"
   },
   "outputs": [],
   "source": [
    "mnist_trainset_reduced.dataset.train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-bkK4ktwfvC"
   },
   "source": [
    "# 1 Vanilla Autoencoder\n",
    "\n",
    "Now, we define the general parameters of our autoencoder. In particular, we note that the latent space is of size 10. We have chosen this since there are 10 digits, but you can change this as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mD56EDzbvUxq"
   },
   "outputs": [],
   "source": [
    "# autoencoder parameters\n",
    "n_rows = mnist_trainset_reduced.dataset.train_data.shape[1]\n",
    "n_cols = mnist_trainset_reduced.dataset.train_data.shape[2]\n",
    "n_channels = 1\n",
    "n_pixels = n_rows*n_cols\n",
    "\n",
    "img_shape = (n_rows, n_cols, n_channels)\n",
    "z_dim = 10\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jLa2-jQwxSI"
   },
   "source": [
    "Now, define the autoencoder architecture by creating a Pytorch class. We will use the following MLP architecture :\n",
    "\n",
    "Encoder :\n",
    "- Flatten input\n",
    "- Dense layer $+$ ReLU\n",
    "- Dense layer $+$ ReLU\n",
    "- Dense layer (no non-linearity), output size ```z_dim```\n",
    "\n",
    "Decoder :\n",
    "- Dense layer $+$ ReLU\n",
    "- Dense layer $+$ ReLU\n",
    "- Dense layer $+$ Sigmoid Activation\n",
    "- Reshape, to size $28\\times 28\\times 1$\n",
    "\n",
    "The intermediate dimensions are referred to as ```h_dim_1``` and ```h_dim2```. For the reshape operation (and even the flatten), you can use the __view__ operation of Pytorch, for example:\n",
    "- ```x.view(-1,n_channels, n_rows,n_cols)```\n",
    "\n",
    "You can use the ```F.relu()``` function for the ReLU non-linearity (or any other choice you like)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEuZfnUlXxMl"
   },
   "outputs": [],
   "source": [
    "class AE(torch.nn.Module ):\n",
    "  def __init__(self, x_dim, h_dim1, h_dim2, z_dim,n_rows,n_cols,n_channels):\n",
    "    super(AE, self).__init__()\n",
    "\n",
    "    self.n_rows = n_rows\n",
    "    self.n_cols = n_cols\n",
    "    self.n_channels = n_channels\n",
    "    self.n_pixels = (self.n_rows)*(self.n_cols)\n",
    "    self.z_dim = z_dim\n",
    "\n",
    "    # encoder part\n",
    "    self.fc1 = ... # FILL IN STUDENT \n",
    "    self.fc2 = ... # FILL IN STUDENT \n",
    "    self.fc3 = ... # FILL IN STUDENT \n",
    "    # decoder part\n",
    "    self.fc4 = ... # FILL IN STUDENT \n",
    "    self.fc5 = ... # FILL IN STUDENT \n",
    "    self.fc6 = ... # FILL IN STUDENT \n",
    "\n",
    "  def encoder(self, x):\n",
    "    h = ... # FILL IN STUDENT \n",
    "    h = ... # FILL IN STUDENT \n",
    "    return ... # FILL IN STUDENT \n",
    "  def decoder(self, z):\n",
    "    h = ... # FILL IN STUDENT \n",
    "    h = ... # FILL IN STUDENT \n",
    "    return ... # FILL IN STUDENT \n",
    "  def forward(self, x):\n",
    "    y = ... # FILL IN STUDENT \n",
    "    return(y)\n",
    "  def loss_function(self,x, y):\n",
    "    bce_loss = ... # FILL IN STUDENT \n",
    "    return torch.mean(bce_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oV40vRMQRoG1"
   },
   "outputs": [],
   "source": [
    "# create an instance of the model\n",
    "ae_dim_1 = 512\n",
    "ae_dim_2 = 256\n",
    "ae_model = AE(x_dim=n_pixels, h_dim1= ae_dim_1, h_dim2=ae_dim_2, z_dim=z_dim,n_rows=n_rows,n_cols=n_cols,n_channels=n_channels)\n",
    "ae_optimizer = optim.Adam(ae_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-659aM36xvXX"
   },
   "source": [
    "Now, define a generic function to train the model for one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wfqX6Brlxjyi"
   },
   "outputs": [],
   "source": [
    "def train_ae(ae_model,data_train_loader,epoch):\n",
    "\ttrain_loss = 0\n",
    "\tfor batch_idx, (data, _) in enumerate(data_train_loader):\n",
    "\t\tae_optimizer.zero_grad()\n",
    "\t\t\n",
    "\t\ty = ... # FILL IN STUDENT \n",
    "\t\tloss_ae = ... # FILL IN STUDENT \n",
    "  \n",
    "\t\tloss_ae.backward()\n",
    "\t\ttrain_loss += loss_ae.item()\n",
    "\t\tae_optimizer.step()\n",
    "\t\t\n",
    "\t\tif batch_idx % 100 == 0:\n",
    "\t\t\tprint('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "\t\t\t\tepoch, batch_idx * len(data), len(data_train_loader.dataset),\n",
    "\t\t\t\t100. * batch_idx / len(data_train_loader), loss_ae.item() / len(data)))\n",
    "\tprint('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(data_train_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3EbmswSzJdK"
   },
   "source": [
    "Finally, train the model for ```n_epochs``` epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11q_0PSibZk-"
   },
   "outputs": [],
   "source": [
    "for epoch in range(0, n_epochs):\n",
    "  train_ae(ae_model,mnist_train_loader,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8jXjdRyzMy2"
   },
   "outputs": [],
   "source": [
    "# some functions to display images and autoencoded images\n",
    "\n",
    "def display_images(imgs):\n",
    "  \n",
    "  r = 1\n",
    "  c = imgs.shape[0]\n",
    "  fig, axs = plt.subplots(r, c)\n",
    "  for j in range(c):\n",
    "    #black and white images\n",
    "    axs[j].imshow(pytorch_to_numpy(imgs[j, 0,:,:]), cmap='gray')\n",
    "    axs[j].axis('off')\n",
    "  plt.show()\n",
    "\n",
    "def display_ae_images(ae_model, test_imgs):\n",
    "  n_images = 5\n",
    "  idx = np.random.randint(0, test_imgs.shape[0], n_images)\n",
    "  test_imgs = test_imgs[idx,:,:,:]\n",
    "  print(test_imgs.shape)\n",
    "\n",
    "  #get output images\n",
    "  output_imgs = pytorch_to_numpy(ae_model.forward( test_imgs ))\n",
    "  print(output_imgs.shape)\n",
    "  \n",
    "  r = 2\n",
    "  c = n_images\n",
    "  fig, axs = plt.subplots(r, c)\n",
    "  for j in range(c):\n",
    "    #black and white images\n",
    "    axs[0,j].imshow(test_imgs[j, 0,:,:], cmap='gray')\n",
    "    axs[0,j].axis('off')\n",
    "    axs[1,j].imshow(output_imgs[j, 0,:,:], cmap='gray')\n",
    "    axs[1,j].axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9pbXch29d68D"
   },
   "outputs": [],
   "source": [
    "test_imgs = next(iter(mnist_train_loader))[0]\n",
    "display_ae_images(ae_model, test_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HP12L-DI12lQ"
   },
   "source": [
    "Do these autoencoded images look good to you ? You can try to change the latent space size to see if the results improve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UqeNhuSdnDt"
   },
   "source": [
    "# 2/ Variational autoencoder\n",
    "\n",
    "Now, we are going to create an variational autoencoder to carry out __image generation__. Let's first recall the idea of a variational autoencoder\n",
    "\n",
    "## Main idea\n",
    "\n",
    "The main idea is to create an autoencoder whose latent codes follow a certain distribution (a Gaussian distribution in practice). This is done with two tools : \n",
    "\n",
    "- A specific architecture, where the encoder produces the average and variance of the latent codes\n",
    "- A specially designed loss function\n",
    "\n",
    "Once the VAE is trained, it is possible to sample in the latent space by producing random normal variables and simply decoding.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "The architecture of the VAE model is as follows:\n",
    "\n",
    "The encoder consists of:\n",
    "\n",
    "Encoder :\n",
    "- Flatten input\n",
    "- Dense layer $+$ ReLU\n",
    "- Dense layer $+$ ReLU\n",
    "- Dense layer (no non-linarity) to produce the average, Dense layer (no non-linarity) to produce the variance (these last two layers are in parallel)\n",
    "\n",
    "Decoder :\n",
    "- Dense layer $+$ ReLU\n",
    "- Dense layer $+$ ReLU\n",
    "- Dense layer $+$ Sigmoid Activation\n",
    "- Reshape, to size $28\\times 28\\times 1$\n",
    "\n",
    "\n",
    "## Variational Autoencoder loss\n",
    "\n",
    "Recall that for the VAE, the loss function is in fact a function to __maximise__. In fact, for implementation, you will see that it is easier to __minimise__ $-\\mathcal{L}$.\n",
    "\n",
    "In the case of an image which is represented by a set of __Bernoulli__ variables (which is relevant for mnist), the original loss function (to maximise) is written :\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L} &= \\log\\left(p_\\theta(x|z)\\right) - KL\\left( q_\\phi(z|x) \\; || \\; p_\\theta(z)\\right) \\\\\n",
    "    &= \\left(\\sum_{i} x_i \\log y_i + (1-x_i) \\log (1-y_i)\\right) - \\left(\\frac{1}{2} \\sum_j \\left( \\sigma_j^2 + \\mu_j^2 - \\log \\sigma_j^2 -1 \\right)\\right)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "where $i$ is summed over the image pixels, and $j$ is summed over the elements of the latent space. $\\sigma_j^2$ is the $j$th element of the latent space variance, and $\\mu_j$ is the $j$th element of the latent space mean.\n",
    "\n",
    "The left part of the loss (reconstruction error) can be implemented simply as the binary cross-entropy between the input x and the output y. Since we are __maximising__ $-$[binary cross-entropy] (look at the formula), this is equivalent to minimising the binary cross-entropy.\n",
    "\n",
    "For the right part of the equation (KL divergence), you need to implement it manually. \n",
    "\n",
    "The final loss is the average, over the batch size, of the sum of the reconstruction error (left part) and the KL divergence (right part). Be careful, in the formula, the sums over $i$ and $j$ are over the number of pixels and the number of latent elements, respectively. To achieve a sum rather than an average, you can use ```torch.nn.BCELoss(reduction='sum')()```, and the ```torch.sum()``` functions.\n",
    "\n",
    "As in the case of the normal autoencoder, you will need to flatten and then reshape the tensors at the beginning/end of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6siMHQLheM4T"
   },
   "outputs": [],
   "source": [
    "class VAE(torch.nn.Module ):\n",
    "  def __init__(self, x_dim, h_dim1, h_dim2, z_dim,n_rows,n_cols,n_channels):\n",
    "    super(VAE, self).__init__()\n",
    "\n",
    "    self.n_rows = n_rows\n",
    "    self.n_cols = n_cols\n",
    "    self.n_channels = n_channels\n",
    "    self.n_pixels = (self.n_rows)*(self.n_cols)\n",
    "    self.z_dim = z_dim\n",
    "\n",
    "    # encoder part\n",
    "    self.fc1 = ... #FILL IN STUDENT\n",
    "    self.fc2 = ... #FILL IN STUDENT\n",
    "    self.fc31 = ... #FILL IN STUDENT\n",
    "    self.fc32 = ... #FILL IN STUDENT\n",
    "    # decoder part\n",
    "    self.fc4 = ... #FILL IN STUDENT\n",
    "    self.fc5 = ... #FILL IN STUDENT\n",
    "    self.fc6 = ... #FILL IN STUDENT\n",
    "\n",
    "  def encoder(self, x):\n",
    "    h = ... #FILL IN STUDENT\n",
    "    h = ... #FILL IN STUDENT\n",
    "    return ... #FILL IN STUDENT (remember, there are two outputs)\n",
    "  def decoder(self, z):\n",
    "    h = ... #FILL IN STUDENT\n",
    "    h = ... #FILL IN STUDENT\n",
    "    return ... #FILL IN STUDENT\n",
    "\n",
    "  def sampling(self, mu, log_var):\n",
    "    # this function samples a Gaussian distribution, with average (mu) and standard deviation specified (using log_var)\n",
    "    std = torch.exp(0.5*log_var)\n",
    "    eps = torch.randn_like(std)\n",
    "    return eps.mul(std).add_(mu) # return z sample\n",
    "\n",
    "  def forward(self, x):\n",
    "    z_mu, z_log_var = ... #FILL IN STUDENT\n",
    "    z = ... #FILL IN STUDENT\n",
    "    return self.decoder(z),z_mu, z_log_var\n",
    "\n",
    "  def loss_function(self,x, y, mu, log_var):\n",
    "    reconstruction_error = ... #FILL IN STUDENT\n",
    "    #reconstruction_error =  torch.nn.MSELoss(reduction='sum')(y.view(-1,self.n_pixels),x.view(-1,self.n_pixels))#F.binary_cross_entropy(y.view(-1,self.n_pixels),x.view(-1,self.n_pixels), reduction='sum')\n",
    "    \n",
    "    KLD = ... #FILL IN STUDENT\n",
    "\n",
    "\n",
    "    return ... #FILL IN STUDENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hk_9fDIphlsi"
   },
   "source": [
    "Now, create the model (similarly as above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVlpC2R3htyU"
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "vae_dim_1 = ae_dim_1\n",
    "vae_dim_2 = ae_dim_2\n",
    "vae_model = VAE(x_dim=n_pixels, h_dim1= vae_dim_1, h_dim2=vae_dim_2, z_dim=z_dim,n_rows=n_rows,n_cols=n_cols,n_channels=n_channels)\n",
    "vae_optimizer = optim.Adam(vae_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYKLF_oMh5HO"
   },
   "source": [
    "Finally, train the model. First modify the training function to the case of the vae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6DjKTWmmssb"
   },
   "outputs": [],
   "source": [
    "def train_vae(vae_model,data_train_loader,epoch):\n",
    "  train_loss = 0\n",
    "  for batch_idx, (data, _) in enumerate(data_train_loader):\n",
    "    vae_optimizer.zero_grad()\n",
    "\n",
    "    y, z_mu, z_log_var = ... #FILL IN STUDENT\n",
    "    loss_vae = ... #FILL IN STUDENT\n",
    "    loss_vae.backward()\n",
    "    train_loss += loss_vae.item()\n",
    "    vae_optimizer.step() \n",
    "\t\t\n",
    "    if batch_idx % 100 == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "      epoch, batch_idx * len(data), len(data_train_loader.dataset),\n",
    "      100. * batch_idx / len(data_train_loader), loss_vae.item() / len(data)))\n",
    "  print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(data_train_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9JUUs6Kh8HB"
   },
   "outputs": [],
   "source": [
    "# now train the model\n",
    "\n",
    "for epoch in range(0, n_epochs):\n",
    "  train_vae(vae_model,mnist_train_loader,epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTfRje_AkKDr"
   },
   "source": [
    "Now, generate some images with the VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41tXdNsFkKk5"
   },
   "outputs": [],
   "source": [
    "def generate_images_vae(vae_model,n_images = 5):\n",
    "\n",
    "  epsilon = torch.randn(n_images,1,vae_model.z_dim)\n",
    "  imgs_generated = vae_model.decoder(epsilon)\n",
    "  return(imgs_generated)\n",
    "\n",
    "imgs_generated = generate_images_vae(vae_model,n_images=5)\n",
    "display_images(imgs_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGnvKoynzaFN"
   },
   "source": [
    "What do you think of the results ? You can try and change the latent space size, or use convolutional layers instead to improve the results."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
